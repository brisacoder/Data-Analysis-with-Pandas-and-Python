# ðŸ“„ Interviewer Handout

### Transformer Attention â€” Whiteboard Exercise (Single Head)

**Goal**
Assess the candidateâ€™s ability to reason about attention from first principles using linear algebra, shapes, and clear explanation.

**What this is NOT**

* Not a PyTorch / TensorFlow API test
* Not about backprop or gradients
* Not about memorizing the paper

**What this IS**

* Shape reasoning
* Linear algebra fluency
* Ability to explain attention clearly
* Conceptual correctness

---

## Constraints (state explicitly)

Write this at the top of the whiteboard:

```
â€¢ Single-head self-attention
â€¢ Batch size: B
â€¢ Sequence length: L
â€¢ Embedding dimension: D
â€¢ No positional encodings
â€¢ Forward pass only
```

---

## Interview Structure (45 minutes total)

| Phase | Topic               | Time      |
| ----- | ------------------- | --------- |
| 1     | Setup & projections | 10â€“12 min |
| 2     | Attention scores    | 8â€“10 min  |
| 3     | Scaling & softmax   | 8â€“10 min  |
| 4     | Output computation  | 8â€“10 min  |
| 5     | Sanity / extension  | 5 min     |

---

## Phase 1 â€” Setup & Projections

**Prompt**

> â€œWe start with an embedded input sequence
> ( X âˆˆ â„^{B Ã— L Ã— D} ).
> Show how to compute queries, keys, and values.â€

**Expected derivation**

```
W_Q, W_K, W_V âˆˆ â„^{D Ã— D}

Q = X W_Q
K = X W_K
V = X W_V
```

**Expected shapes**

```
Q, K, V âˆˆ â„^{B Ã— L Ã— D}
```

**What youâ€™re looking for**

* Correct shape propagation
* Clear explanation of *why* Q, K, V are separate
* Comfort ignoring batch temporarily (good sign)

---

## Phase 2 â€” Attention Scores

**Prompt**

> â€œFor a single batch element, compute the attention scores.
> What operation do we use and what shape do we get?â€

**Expected**

```
Fix b:

Q âˆˆ â„^{L Ã— D}
K âˆˆ â„^{L Ã— D}

S = Q Káµ€ âˆˆ â„^{L Ã— L}
```

**Interpretation**

```
S_ij = âŸ¨ q_i , k_j âŸ©
```

Candidate should say something like:

> â€œThis computes similarity between every query token and every key token.â€

ðŸš© Red flag if they canâ€™t explain why this is LÃ—L.

---

## Phase 3 â€” Scaling & Softmax

**Prompt**

> â€œHow do we convert scores into probabilities?
> Why do we scale?â€

**Expected**

```
S_scaled = S / âˆšD
A = softmax(S_scaled, axis=1)
```

**Properties**

```
A âˆˆ â„^{L Ã— L}
âˆ‘_j A_ij = 1
```

**Acceptable intuition**

* Prevents large dot products
* Keeps softmax gradients stable

ðŸš© Red flag: â€œbecause the paper says soâ€

---

## Phase 4 â€” Output Computation

**Prompt**

> â€œHow do we compute the output representations?â€

**Expected**

```
Y = A V
```

**Shapes**

```
(L Ã— L) @ (L Ã— D) â†’ (L Ã— D)
```

**Expanded**

```
y_i = Î£_j A_ij Â· v_j
```

ðŸ’¡ This sentence is gold:

> â€œEach output token is a weighted sum of value vectors.â€

---

## Phase 5 â€” Optional Extension (pick ONE)

Choose based on time:

* â€œWhere would causal masking apply?â€
* â€œWhat changes for cross-attention?â€
* â€œHow does multi-head attention differ conceptually?â€
* â€œWhy is attention permutation-equivariant?â€

Do **not** ask multiple.

---

# ðŸ§  Expected Whiteboard Diagram (Canonical)

This is what a *strong* candidateâ€™s board typically converges to.

```
X : (B, L, D)
â”‚
â”œâ”€â”€ W_Q â”€â”€â–¶ Q : (B, L, D)
â”œâ”€â”€ W_K â”€â”€â–¶ K : (B, L, D)
â””â”€â”€ W_V â”€â”€â–¶ V : (B, L, D)

(for one batch)

Q : (L, D)
K : (L, D)
V : (L, D)

S = Q Káµ€
S : (L, L)

A = softmax(S / âˆšD)
A : (L, L)

Y = A V
Y : (L, D)
```

If they draw arrows and label shapes clearly â†’ **strong signal**.

---

# ðŸ“Š Grading Rubric (Simple & Effective)

| Category          | Strong                | Weak               |
| ----------------- | --------------------- | ------------------ |
| Shape reasoning   | Immediate, consistent | Frequent confusion |
| Linear algebra    | Clean, correct        | Hand-wavy          |
| Explanation       | Intuitive & precise   | Symbol dumping     |
| Scaling intuition | Correct               | Memorized          |
| Time management   | Finishes              | Gets stuck early   |

---

## Strong Candidate Signals (Green Flags)

* â€œLetâ€™s ignore batch for clarityâ€
* Writes shapes unprompted
* Explains dot products intuitively
* States weighted-sum interpretation
* Notices quadratic complexity

## Weak Signals (Red Flags)

* Confuses L vs D repeatedly
* Cannot explain softmax axis
* Treats attention as magic
* Needs hints for every step

---

# ðŸ§© Interviewer Rescue Prompts (Use Sparingly)

If candidate stalls:

* â€œWhat shape do you want the output to be?â€
* â€œHow many tokens are interacting here?â€
* â€œThink pairwise similarities.â€

These preserve signal without giving answers.

---

## Final Interviewer Guidance

Say this once at the start:

> â€œThis is collaborative â€” Iâ€™m evaluating your reasoning, not speed.â€

That one sentence dramatically improves candidate performance **without reducing signal**.

--- 

Complete NUmeric Example

---

# ðŸ”¢ Numeric Attention Example (Single-Head, Tiny Sentence)

## Setup (you write this on the board)

> Sentence length **L = 2**
> Embedding dimension **D = 2**
> Single-head self-attention
> Ignore batch dimension

Sentence (purely symbolic):

```
["I", "code"]
```

---

## Step 1 â€” Input embeddings

Give them **explicit numbers** (critical for timing):

```
X =
[ 1  0 ]   â† token 1 ("I")
[ 0  1 ]   â† token 2 ("code")
```

So:

```
X âˆˆ â„^{2 Ã— 2}
```

---

## Step 2 â€” Projection matrices (identity on purpose)

Tell the candidate:

> â€œLetâ€™s make the projections trivial so we can focus on attention.â€

```
W_Q = W_K = W_V = Iâ‚‚
```

Therefore:

```
Q = X
K = X
V = X
```

Still:

```
Q, K, V âˆˆ â„^{2 Ã— 2}
```

---

## Step 3 â€” Compute attention scores

Ask:

> â€œCompute the attention score matrix S = QKáµ€.â€

They should compute:

```
Káµ€ =
[ 1  0 ]
[ 0  1 ]

S = QKáµ€ =
[ 1Â·1 + 0Â·0   1Â·0 + 0Â·1 ] = [ 1  0 ]
[ 0Â·1 + 1Â·0   0Â·0 + 1Â·1 ]   [ 0  1 ]
```

So:

```
S =
[ 1  0 ]
[ 0  1 ]
```

Shape check:

```
S âˆˆ â„^{2 Ã— 2}
```

Interpretation (they should say this):

* Token 1 attends most to itself
* Token 2 attends most to itself

---

## Step 4 â€” Scaling

Tell them explicitly:

```
âˆšD = âˆš2 â‰ˆ 1.414
```

Scaled scores:

```
S_scaled =
[ 1/âˆš2   0     ]
[ 0      1/âˆš2  ]
â‰ˆ
[ 0.707  0     ]
[ 0      0.707 ]
```

---

## Step 5 â€” Softmax (row-wise)

Ask:

> â€œApply softmax row by row.â€

Row 1:

```
softmax([0.707, 0]) =
[ e^0.707 / (e^0.707 + 1),
  1        / (e^0.707 + 1) ]
â‰ˆ [0.67, 0.33]
```

Row 2:

```
softmax([0, 0.707]) â‰ˆ [0.33, 0.67]
```

So attention matrix:

```
A =
[ 0.67  0.33 ]
[ 0.33  0.67 ]
```

Key property to emphasize:

```
Each row sums to 1
```

---

## Step 6 â€” Output computation

Ask:

> â€œNow compute Y = A V.â€

Recall:

```
V =
[ 1  0 ]
[ 0  1 ]
```

Row 1:

```
yâ‚ = 0.67Â·[1,0] + 0.33Â·[0,1]
   = [0.67, 0.33]
```

Row 2:

```
yâ‚‚ = 0.33Â·[1,0] + 0.67Â·[0,1]
   = [0.33, 0.67]
```

Final output:

```
Y =
[ 0.67  0.33 ]
[ 0.33  0.67 ]
```

Shape:

```
Y âˆˆ â„^{2 Ã— 2}
```

---

## Step 7 â€” Interpretation (this is the signal)

A strong candidate will say:

> â€œEach output token is a weighted mixture of both value vectors, biased toward itself.â€

If they say this unprompted â€” **excellent signal**.

---

## Optional follow-ups (pick ONE)

If time remains:

1. **Causal mask**

   * Mask upper-right element of `S`
   * Ask what changes in `A`

2. **Change embeddings**

   * Make both tokens identical
   * Ask what happens to attention

3. **Why this isnâ€™t just averaging**

   * Let them explain adaptivity

---

## Why this example works

âœ… Minimal arithmetic
âœ… No matrix larger than 2Ã—2
âœ… Exercises **every step of attention**
âœ… Easy to debug mistakes
âœ… Whiteboard-friendly
âœ… Scales naturally to multi-head discussion


---

# Transformer Self-Attention â€” Theory Worksheet (with Full Answers)

> **Single-head self-attention, forward pass only**
> This document contains the *fully worked theoretical derivation* and is intended for **interviewer reference**.

---

## Assumptions

* Single-head self-attention
* Batch size: ( B )
* Sequence length: ( L )
* Embedding dimension: ( D )
* No positional encodings
* No gradients or backpropagation

---

## 1. Input Representation

Let the embedded input sequence be:

$$
\mathbf{X} \in \mathbb{R}^{B \times L \times D}
$$

Each batch element contains a sequence of ( L ) tokens, each represented by a ( D )-dimensional embedding.

---

## 2. Linear Projections (Queries, Keys, Values)

We introduce three learned projection matrices:

$$
\mathbf{W}_Q,; \mathbf{W}_K,; \mathbf{W}_V \in \mathbb{R}^{D \times D}
$$

The query, key, and value tensors are computed as:

$$
\mathbf{Q} = \mathbf{X} \cdot \mathbf{W}_Q
$$

$$
\mathbf{K} = \mathbf{X} \cdot \mathbf{W}_K
$$

$$
\mathbf{V} = \mathbf{X} \cdot \mathbf{W}_V
$$

### Shapes

$$
\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{B \times L \times D}
$$

Each token now has **three distinct representations**, enabling asymmetric similarity computation.

---

## 3. Attention Score Matrix

To simplify reasoning, fix a batch element and drop the batch dimension:

$$
\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{L \times D}
$$

The attention score matrix is computed via matrix multiplication:

$$
\mathbf{S} = \mathbf{Q} \cdot \mathbf{K}^{\top}
$$

### Shape

$$
\mathbf{S} \in \mathbb{R}^{L \times L}
$$

### Element-wise Interpretation

$$
S_{ij} = \langle \mathbf{q}_i, \mathbf{k}_j \rangle
$$

Each entry represents the dot-product similarity between:

* the **query** vector of token ( i )
* the **key** vector of token ( j )

This computes **all pairwise token interactions**.

---

## 4. Scaling and Softmax

To stabilize the softmax operation, the score matrix is scaled:

$$
\mathbf{S}_{\text{scaled}} = \frac{\mathbf{S}}{\sqrt{D}}
$$

The attention matrix is then obtained by applying softmax **row-wise**:

$$
\mathbf{A} = \operatorname{softmax}!\left( \mathbf{S}_{\text{scaled}} \right)
$$

### Shape

$$
\mathbf{A} \in \mathbb{R}^{L \times L}
$$

### Key Property

For every query position ( i ):

$$
\sum_{j=1}^{L} A_{ij} = 1
$$

Each row of ( \mathbf{A} ) forms a **probability distribution over tokens**.

---

## 5. Output Computation

The output of attention is computed as:

$$
\mathbf{Y} = \mathbf{A} \cdot \mathbf{V}
$$

### Shape

$$
\mathbf{Y} \in \mathbb{R}^{L \times D}
$$

### Expanded Form

For each output token ( i ):

$$
\mathbf{y}*i = \sum*{j=1}^{L} A_{ij} , \mathbf{v}_j
$$

Each output vector is a **weighted sum of value vectors**.

---

## 6. Interpretation

* Each output token representation is **contextualized**
* Information from all tokens is incorporated
* Weights are determined dynamically via learned similarity
* The mechanism is **permutation-equivariant** (in the absence of positional encoding)

---

## 7. One-Sentence Summary (Canonical)

> *Single-head self-attention computes all pairwise token similarities, normalizes them into probability distributions, and uses them to form weighted sums of value vectors, producing context-aware token representations.*


